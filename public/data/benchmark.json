[
	{
		"id": "025ca0fe-f04c-4d17-9f4c-9d1aca6666bd",
		"leaderboard": "263e7d41-65a5-4b7b-8f48-5c7b94ca916b",
		"name": "RxR-CE",
		"description": "Beyond the Nav-Graph: Vision and Language Navigation in Continuous Environments\n",
		"organization": "Oregon State University, Georgia Institute of Technology, Facebook AI Research",
		"released": {
			"at": {
				"year": 2020,
				"month": 4,
				"date": 6
			}
		},
		"repository": "https://github.com/jacobkrantz/VLN-CE",
		"huggingface": null,
		"website": "https://jacobkrantz.github.io/vlnce/",
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Instruction Navigation"
		],
		"properties": {
			"TotalSR": {
				"order": 0,
				"index": 0,
				"label": "Total SR",
				"unit": null
			},
			"InstructionNavigation": {
				"order": 1,
				"index": 1,
				"label": "Instruction Navigation",
				"unit": null
			}
		}
	},
	{
		"id": "034bf6b5-dc18-45da-bfaf-337589125632",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "UniEQA",
		"description": "UniEQA & UniEval: A Unified Benchmark and Evaluation Platform for Multimodal Foundation Models in Embodied Question Answering\n",
		"organization": "Tianjin University",
		"released": {
			"at": {
				"year": 2025,
				"month": 5,
				"date": null
			}
		},
		"repository": "https://github.com/TJURL-Lab/UniEQA",
		"huggingface": "https://huggingface.co/datasets/TJURL-Lab/UniEQA",
		"website": null,
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Object Type",
			"Object Property",
			"Object State",
			"Spatial Relationship",
			"Temporal Description",
			"Temporal Order",
			"General Knowledge",
			"Affordance Prediction",
			"Object Reasoning",
			"Task Reasoning",
			"Location Navigation"
		],
		"properties": {
			"ObjectType": {
				"order": 1,
				"index": 0,
				"label": "Object Type",
				"unit": null
			},
			"ObjectProperty": {
				"order": 2,
				"index": 1,
				"label": "Object Property",
				"unit": null
			},
			"ObjectState": {
				"order": 3,
				"index": 2,
				"label": "Object State",
				"unit": null
			},
			"SpatialPerception": {
				"order": 4,
				"index": 3,
				"label": "Spatial Perception",
				"unit": null
			},
			"ActionPerception": {
				"order": 5,
				"index": 4,
				"label": "Action Perception",
				"unit": null
			},
			"TemporalPerception": {
				"order": 6,
				"index": 5,
				"label": "Temporal Perception",
				"unit": null
			},
			"Affordance": {
				"order": 7,
				"index": 6,
				"label": "Affordance",
				"unit": null
			},
			"WorldKnowledge": {
				"order": 8,
				"index": 7,
				"label": "World Knowledge",
				"unit": null
			},
			"TaskRelatedObjectReasoning": {
				"order": 9,
				"index": 8,
				"label": "Task-Related Object Reasoning",
				"unit": null
			},
			"SituatedReasoning": {
				"order": 10,
				"index": 9,
				"label": "Situated Reasoning",
				"unit": null
			},
			"ClosedLoopPlanning": {
				"order": 11,
				"index": 10,
				"label": "Closed-Loop Planning",
				"unit": null
			},
			"OpenLoopPlanning": {
				"order": 12,
				"index": 11,
				"label": "Open-Loop Planning",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 12,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "089294d0-59db-47f0-a543-19d8e3708448",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "VABench-Point",
		"description": "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation\n",
		"organization": "Tianjin University",
		"released": {
			"at": {
				"year": 2025,
				"month": 5,
				"date": 10
			}
		},
		"repository": "https://github.com/pickxiguapi/Embodied-FSD",
		"huggingface": "https://huggingface.co/collections/IffYuan/fsd-683fa0d552e70f302fd04b34",
		"website": "https://embodied-fsd.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Spatial Reasoning"
		],
		"properties": {
			"Spatial Reasoning": {
				"order": 1,
				"index": 0,
				"label": "Spatial Reasoning",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 1,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "1961aa46-9085-42ed-b384-d9ffe52921a8",
		"leaderboard": "1143b13b-2754-4660-9f79-d0d0dc1f273e",
		"name": "EB-ALFRED",
		"description": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents\n",
		"organization": "University of Illinois Urbana-Champaign, Northwestern University, University of Toronto, Toyota Technological Institute at Chicago",
		"released": {
			"at": {
				"year": 2025,
				"month": 2,
				"date": null
			}
		},
		"repository": "https://github.com/EmbodiedBench/EmbodiedBench",
		"huggingface": null,
		"website": "https://embodiedbench.github.io/",
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Basic Planning",
			"Visual Reference Planning",
			"Spatial Reference Planning",
			"Knowledge Reference Planning"
		],
		"properties": {
			"BaseCapability": {
				"order": 1,
				"index": 0,
				"label": "Base Capability",
				"unit": null
			},
			"CommonSense": {
				"order": 2,
				"index": 1,
				"label": "Common Sense",
				"unit": null
			},
			"ComplexInstruction": {
				"order": 3,
				"index": 2,
				"label": "Complex Instruction",
				"unit": null
			},
			"VisualAppearance": {
				"order": 4,
				"index": 3,
				"label": "Visual Appearance",
				"unit": null
			},
			"SpatialAwareness": {
				"order": 5,
				"index": 4,
				"label": "Spatial Awareness",
				"unit": null
			},
			"LongHorizon": {
				"order": 6,
				"index": 5,
				"label": "Long Horizon",
				"unit": null
			},
			"TotalSR": {
				"order": 0,
				"index": 6,
				"label": "Total SR",
				"unit": null
			}
		}
	},
	{
		"id": "1c6ba4e5-1555-487c-a4bc-3ee0c18b8b09",
		"leaderboard": "263e7d41-65a5-4b7b-8f48-5c7b94ca916b",
		"name": "R2R-CE",
		"description": "Beyond the Nav-Graph: Vision and Language Navigation in Continuous Environments\n",
		"organization": "Oregon State University, Georgia Institute of Technology, Facebook AI Research",
		"released": {
			"at": {
				"year": 2020,
				"month": 4,
				"date": 6
			}
		},
		"repository": "https://github.com/jacobkrantz/VLN-CE",
		"huggingface": null,
		"website": "https://jacobkrantz.github.io/vlnce/",
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Instruction Navigation"
		],
		"properties": {
			"TotalSR": {
				"order": 0,
				"index": 0,
				"label": "Total SR",
				"unit": null
			},
			"InstructionNavigation": {
				"order": 1,
				"index": 1,
				"label": "Instruction Navigation",
				"unit": null
			}
		}
	},
	{
		"id": "25206b0f-a955-4e39-86cf-53dcc9528bfc",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "VSI",
		"description": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces\n",
		"organization": "New York University",
		"released": {
			"at": {
				"year": 2024,
				"month": 12,
				"date": 18
			}
		},
		"repository": "https://github.com/vision-x-nyu/thinking-in-space",
		"huggingface": null,
		"website": "https://vision-x-nyu.github.io/thinking-in-space.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Object Property",
			"Object Count",
			"Spatial Relationship",
			"Spatial Distance",
			"Spatial Size",
			"Temporal Order",
			"Basic Planning"
		],
		"properties": {
			"ObjectCount": {
				"order": 1,
				"index": 0,
				"label": "Object Count",
				"unit": null
			},
			"AbsoluteDistance": {
				"order": 2,
				"index": 1,
				"label": "Absolute Distance",
				"unit": null
			},
			"ObjectSize": {
				"order": 3,
				"index": 2,
				"label": "Object Size",
				"unit": null
			},
			"RoomSize": {
				"order": 4,
				"index": 3,
				"label": "Room Size",
				"unit": null
			},
			"RelativeDistance": {
				"order": 5,
				"index": 4,
				"label": "Relative Distance",
				"unit": null
			},
			"RelativeDirection": {
				"order": 6,
				"index": 5,
				"label": "Relative Direction",
				"unit": null
			},
			"RoutePlan": {
				"order": 7,
				"index": 6,
				"label": "Route Plan",
				"unit": null
			},
			"AppearanceOrder": {
				"order": 8,
				"index": 7,
				"label": "Appearance Order",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 8,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "27e481d6-6057-49dc-ad80-529f9127245f",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "PhyBlock",
		"description": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly\n",
		"organization": "Sun Yat-sen University",
		"released": {
			"at": {
				"year": 2025,
				"month": 5,
				"date": 26
			}
		},
		"repository": "https://github.com/PhyBlock/PhyBlock",
		"huggingface": "https://huggingface.co/datasets/PhyBlock/PhyBlock_Benchmark",
		"website": "https://phyblock.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Object Type",
			"Object Property",
			"Object Count",
			"Spatial Relationship",
			"Spatial Distance",
			"Temporal Order",
			"Affordance Prediction",
			"Spatial Reasoning",
			"Knowledge Reasoning",
			"Task Reasoning"
		],
		"properties": {
			"Spatial Reference Planning": {
				"order": 1,
				"index": 0,
				"label": "Spatial Reference Planning",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 1,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "5df5ca38-7ca9-4d35-81c6-3dfe52700755",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "ERQA",
		"description": "Gemini Robotics: Bringing AI into the Physical World\n",
		"organization": "Google",
		"released": {
			"at": {
				"year": 2025,
				"month": 3,
				"date": null
			}
		},
		"repository": "https://github.com/embodiedreasoning/ERQA",
		"huggingface": null,
		"website": "https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Object Type",
			"Object State",
			"Spatial Relationship",
			"Spatial Reasoning",
			"Temporal Reasoning",
			"Task Reasoning"
		],
		"properties": {
			"ActionReasoning": {
				"order": 1,
				"index": 0,
				"label": "Action Reasoning",
				"unit": null
			},
			"MultiviewReasoning": {
				"order": 2,
				"index": 1,
				"label": "Multi-view Reasoning",
				"unit": null
			},
			"Pointing": {
				"order": 3,
				"index": 2,
				"label": "Pointing",
				"unit": null
			},
			"SpatialReasoning": {
				"order": 4,
				"index": 3,
				"label": "Spatial Reasoning",
				"unit": null
			},
			"StateEstimation": {
				"order": 5,
				"index": 4,
				"label": "State Estimation",
				"unit": null
			},
			"TaskReasoning": {
				"order": 6,
				"index": 5,
				"label": "Task Reasoning",
				"unit": null
			},
			"TrajectoryReasoning": {
				"order": 7,
				"index": 6,
				"label": "Trajectory Reasoning",
				"unit": null
			},
			"Other": {
				"order": 8,
				"index": 7,
				"label": "Other",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 8,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "68bf8b5b-d8d4-49d7-9e7e-03e4d547189b",
		"leaderboard": "1143b13b-2754-4660-9f79-d0d0dc1f273e",
		"name": "ET-Plan-Bench",
		"description": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards Spatial-Temporal Cognition with Foundation Models\n",
		"organization": "Huawei Noah’s Ark Lab, Huawei Cloud",
		"released": {
			"at": {
				"year": 2025,
				"month": 2,
				"date": null
			}
		},
		"repository": null,
		"huggingface": null,
		"website": null,
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Spatial Reference Planning",
			"Temporal Reference Planning"
		],
		"properties": {
			"NaviLayoutMap": {
				"order": 1,
				"index": 0,
				"label": "Navi + Layout Map",
				"unit": ""
			},
			"Navi": {
				"order": 2,
				"index": 1,
				"label": "Navi",
				"unit": ""
			},
			"NaviRelation": {
				"order": 3,
				"index": 2,
				"label": "Navi + Relation",
				"unit": ""
			},
			"NaviManiMultiObjectsTemporalConstraints": {
				"order": 4,
				"index": 3,
				"label": "Navi & Mani + Multi Objects + Temporal Constraints",
				"unit": ""
			},
			"TotalSR": {
				"order": 0,
				"index": 4,
				"label": "Total SR",
				"unit": ""
			}
		}
	},
	{
		"id": "73de273a-7d38-40c0-b869-2b2e6156c897",
		"leaderboard": "263e7d41-65a5-4b7b-8f48-5c7b94ca916b",
		"name": "EB-Navigation",
		"description": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents\n",
		"organization": "University of Illinois Urbana-Champaign, Northwestern University, University of Toronto, Toyota Technological Institute at Chicago",
		"released": {
			"at": {
				"year": 2025,
				"month": 2,
				"date": null
			}
		},
		"repository": "https://github.com/EmbodiedBench/EmbodiedBench",
		"huggingface": null,
		"website": "https://embodiedbench.github.io/",
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Object Navigation"
		],
		"properties": {
			"BaseCapability": {
				"order": 1,
				"index": 0,
				"label": "Base Capability",
				"unit": null
			},
			"CommonSense": {
				"order": 2,
				"index": 1,
				"label": "Common Sense",
				"unit": null
			},
			"ComplexInstruction": {
				"order": 3,
				"index": 2,
				"label": "Complex Instruction",
				"unit": null
			},
			"VisualAppearance": {
				"order": 4,
				"index": 3,
				"label": "Visual Appearance",
				"unit": null
			},
			"LongHorizon": {
				"order": 5,
				"index": 4,
				"label": "Long Horizon",
				"unit": null
			},
			"TotalSR": {
				"order": 0,
				"index": 5,
				"label": "Total SR",
				"unit": null
			}
		}
	},
	{
		"id": "8b56c6cf-becf-4bef-b612-1804d2f20cda",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "SQA3D",
		"description": "SQA3D: Situated Question Answering in 3D Scenes\n",
		"organization": "Beijing Institute for General Artificial Intelligence (BIGAI), UCLA, Tsinghua University, Peking University",
		"released": {
			"at": {
				"year": 2023,
				"month": 1,
				"date": 30
			}
		},
		"repository": "https://github.com/SilongYong/SQA3D",
		"huggingface": null,
		"website": "https://sqa3d.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Spatial Reasoning"
		],
		"properties": {
			"EM": {
				"order": 1,
				"index": 0,
				"label": "EM",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 1,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "a9228f2e-3191-4a08-8f03-64e73cae2e78",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "Where2Place",
		"description": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics\n",
		"organization": "University of Washington",
		"released": {
			"at": {
				"year": 2024,
				"month": 10,
				"date": 4
			}
		},
		"repository": "https://github.com/wentaoyuan/RoboPoint",
		"huggingface": null,
		"website": "https://robo-point.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Affordance Prediction"
		],
		"properties": {
			"Total": {
				"order": 0,
				"index": 0,
				"label": "Total Score",
				"unit": null
			},
			"AffordancePrediction": {
				"order": 1,
				"index": 1,
				"label": "Affordance Prediction",
				"unit": null
			}
		}
	},
	{
		"id": "b9a18add-a3fb-4893-89e3-ddeea8053ce2",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "MineAnyBuild",
		"description": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents\n",
		"organization": "Sun Yat-sen University",
		"released": {
			"at": {
				"year": 2025,
				"month": 5,
				"date": 26
			}
		},
		"repository": "https://github.com/MineAnyBuild/MineAnyBuild/tree/main",
		"huggingface": "https://huggingface.co/datasets/SaDil/MineAnyBuild",
		"website": "https://mineanybuild.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Spatial Reasoning",
			"Knowledge Reasoning"
		],
		"properties": {
			"Spatial Reasoning": {
				"order": 1,
				"index": 0,
				"label": "Spatial Reasoning",
				"unit": null
			},
			"Spatial Commonsense": {
				"order": 2,
				"index": 1,
				"label": "Spatial Commonsense",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 2,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "ba0be521-6b10-4881-964c-97cd4d2d6070",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "OpenEQA",
		"description": "OpenEQA: Embodied Question Answering in the Era of Foundation Models\n",
		"organization": "Meta",
		"released": {
			"at": {
				"year": 2024,
				"month": null,
				"date": null
			}
		},
		"repository": "https://github.com/facebookresearch/open-eqa",
		"huggingface": null,
		"website": "https://open-eqa.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Object Type",
			"Object Property",
			"Object State",
			"Spatial Localization",
			"General Knowledge",
			"Spatial Reasoning",
			"Knowledge Reasoning"
		],
		"properties": {
			"ObjectType": {
				"order": 1,
				"index": 0,
				"label": "Object Recognition",
				"unit": null
			},
			"ObjectProperty": {
				"order": 2,
				"index": 1,
				"label": "Attribute Recognition",
				"unit": null
			},
			"ObjectState": {
				"order": 3,
				"index": 2,
				"label": "Object State Recognition",
				"unit": null
			},
			"Affordance": {
				"order": 4,
				"index": 3,
				"label": "Functional Reasoning",
				"unit": null
			},
			"WorldKnowledge": {
				"order": 5,
				"index": 4,
				"label": "World Knowledge",
				"unit": null
			},
			"ObjectLocalization": {
				"order": 6,
				"index": 5,
				"label": "Object Localization",
				"unit": null
			},
			"SpatialReasoning": {
				"order": 7,
				"index": 6,
				"label": "Spatial Reasoning",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 7,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "cbb64d1c-4377-4231-a465-73ffb10972c5",
		"leaderboard": "1143b13b-2754-4660-9f79-d0d0dc1f273e",
		"name": "EB-Habitat",
		"description": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents\n",
		"organization": "University of Illinois Urbana-Champaign, Northwestern University, University of Toronto, Toyota Technological Institute at Chicago",
		"released": {
			"at": {
				"year": 2025,
				"month": 2,
				"date": null
			}
		},
		"repository": "https://github.com/EmbodiedBench/EmbodiedBench",
		"huggingface": null,
		"website": "https://embodiedbench.github.io/",
		"default": {
			"property": "TotalSR"
		},
		"capabilities": [
			"Basic Planning",
			"Visual Reference Planning",
			"Spatial Reference Planning",
			"Knowledge Reference Planning"
		],
		"properties": {
			"BaseCapability": {
				"order": 1,
				"index": 0,
				"label": "Base Capability",
				"unit": null
			},
			"CommonSense": {
				"order": 2,
				"index": 1,
				"label": "Common Sense",
				"unit": null
			},
			"ComplexInstruction": {
				"order": 3,
				"index": 2,
				"label": "Complex Instruction",
				"unit": null
			},
			"VisualAppearance": {
				"order": 4,
				"index": 3,
				"label": "Visual Appearance",
				"unit": null
			},
			"SpatialAwareness": {
				"order": 5,
				"index": 4,
				"label": "Spatial Awareness",
				"unit": null
			},
			"LongHorizon": {
				"order": 6,
				"index": 5,
				"label": "Long Horizon",
				"unit": null
			},
			"TotalSR": {
				"order": 0,
				"index": 6,
				"label": "Total SR",
				"unit": null
			}
		}
	},
	{
		"id": "df46aa1a-099f-4841-bc02-ba9608e18c7d",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "Scan2Cap",
		"description": "Scan2Cap: Context-aware Dense Captioning in RGB-D Scans\n",
		"organization": "Technical University of Munich, Simon Fraser University",
		"released": {
			"at": {
				"year": 2020,
				"month": 12,
				"date": 7
			}
		},
		"repository": "https://github.com/daveredrum/Scan2Cap",
		"huggingface": null,
		"website": "https://daveredrum.github.io/Scan2Cap/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Spatial Relationship"
		],
		"properties": {
			"CIDEr": {
				"order": 1,
				"index": 0,
				"label": "CIDEr",
				"unit": null
			},
			"B4": {
				"order": 2,
				"index": 1,
				"label": "B-4",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 2,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "e0465aba-17b0-4f34-9aa4-345c9bacdac2",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "ScanQA",
		"description": "ScanQA: 3D Question Answering for Spatial Scene Understanding\n",
		"organization": "Kyoto University, ATR, RIKEN AIP, etc",
		"released": {
			"at": {
				"year": 2022,
				"month": 5,
				"date": 1
			}
		},
		"repository": "https://github.com/ATR-DBI/ScanQA",
		"huggingface": null,
		"website": null,
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Spatial Localization"
		],
		"properties": {
			"CIDEr": {
				"order": 1,
				"index": 0,
				"label": "CIDEr",
				"unit": null
			},
			"EM": {
				"order": 2,
				"index": 1,
				"label": "EM",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 2,
				"label": "Total Score",
				"unit": null
			}
		}
	},
	{
		"id": "ed3fc303-b421-4e73-936a-e93a5aada777",
		"leaderboard": "09b4a56a-2e41-4103-a330-129381c24450",
		"name": "RoboVQA",
		"description": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics\n",
		"organization": "Google DeepMind",
		"released": {
			"at": {
				"year": 2023,
				"month": 11,
				"date": 1
			}
		},
		"repository": "https://github.com/google-deepmind/robovqa/tree/main",
		"huggingface": null,
		"website": "https://robovqa.github.io/",
		"default": {
			"property": "Total"
		},
		"capabilities": [
			"Temporal Description",
			"Affordance Prediction",
			"Task Reasoning",
			"Basic Planning"
		],
		"properties": {
			"BELU-1": {
				"order": 1,
				"index": 0,
				"label": "BELU-1",
				"unit": null
			},
			"BELU-2": {
				"order": 2,
				"index": 1,
				"label": "BELU-2",
				"unit": null
			},
			"BELU-3": {
				"order": 3,
				"index": 2,
				"label": "BELU-3",
				"unit": null
			},
			"BELU-4": {
				"order": 4,
				"index": 3,
				"label": "BELU-4",
				"unit": null
			},
			"Total": {
				"order": 0,
				"index": 4,
				"label": "Total Score",
				"unit": null
			}
		}
	}
]