Scan2Cap is a benchmark for dense captioning in 3D scans captured
using commodity RGB-D sensors. It focuses on the task of generating natural
language descriptions for multiple objects within a 3D scene, given as a
point cloud. Each object is annotated with a 3D bounding box and an
associated textual description, enabling the joint evaluation of object
localization and language generation. The benchmark emphasizes detailed scene
understanding by requiring models to capture both the geometric properties of
objects and their contextual relationships. Built on the ScanRefer dataset,
Scan2Cap provides a challenging testbed for evaluating multimodal models that
combine 3D perception with language capabilities.

Reference:  
Chen Z, Gholami A, Nie√üner M, et al. Scan2cap: Context-aware dense captioning in rgb-d scans[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 3193-3203.
