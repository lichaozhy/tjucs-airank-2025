VSI-Bench quantitatively evaluates the visual-spatial intelligence of
MLLMs from egocentric video. VSI-Bench comprises over 5,000 question-answer
pairs derived from 288 real videos. These videos are sourced from the
validation sets of the public indoor 3D scene reconstruction datasets
ScanNet, ScanNet++, and ARKitScenes, and represent diverse environments --
including residential spaces, professional settings (e.g., offices, labs),
and industrial spaces (e.g., factories) and multiple geographic regions. By
repurposing these existing 3D reconstruction and understanding datasets,
VSI-Bench benefits from accurate object-level annotations, which are used in
question generation and could support future studies exploring the connection
between MLLMs and 3D reconstruction.

Reference:  
Yang J, Yang S, Gupta A W, et al. Thinking in space: How multimodal large language models see, remember, and recall spaces[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 10632-10643.
