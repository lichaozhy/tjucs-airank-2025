The EA-Temporal-Auto-20251020 benchmark provides a quantitative evaluation of the temporal reasoning capabilities of Multimodal Large Language Models (MLLMs) through first-person view image sequences. The benchmark comprises over 500 question-answer pairs derived from embodied agent trajectory data generated based on the ALFRED dataset, with secondary annotation and verification conducted by human experts.
<div class="text-caption">

Reference:<br>
Embodied Arena Team. EmbodiedArena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI[J/OL]. arXiv preprint arXiv:2509.15273, 2025.
</div>
