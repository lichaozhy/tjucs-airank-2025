EmbSpatial-Bench, a dedicated evaluation benchmark for embodied artificial intelligence, is proposed to systematically assess the spatial understanding capabilities of large vision-language models (LVLMs) in embodied environments. The benchmark focuses on six fundamental egocentric spatial relationships—left, right, above, below, close, and far—covering horizontal, vertical, and depth dimensions that are critical for embodied perception and interaction. EmbSpatial-Bench is constructed from three heterogeneous embodied 3D datasets, including Matterport3D, AI2-THOR, and ScanNet, spanning both simulated and real-world indoor scenarios. It comprises 3,640 carefully curated multiple-choice question–answer pairs, covering 294 object categories across 277 scenes. By automatically extracting spatial relations from well-annotated 3D environments and applying rigorous filtering and human verification, EmbSpatial-Bench ensures high reliability, balanced relation distribution, and strong alignment with real embodied task settings.

<div class="text-caption">

Reference:<br>
Du M, Wu B, Li Z, et al. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models[C]//Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2024: 346-355.

</div>