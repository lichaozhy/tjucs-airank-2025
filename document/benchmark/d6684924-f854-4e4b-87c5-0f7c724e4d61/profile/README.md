Multi-modal Situated Question Answering (MSQA) is a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide both texts, images, and point clouds for situation and question description, aiming to resolve ambiguity in describing situations with single-modality inputs (e.g., texts).

<div class="text-caption">

Reference:<br>
Linghu X, Huang J, Niu X, et al. Multi-modal situated reasoning in 3d scenes[J]. Advances in Neural Information Processing Systems, 2024, 37: 140903-140936.

</div>
